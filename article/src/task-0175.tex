\section{Существующие решения}

Проблема выявления токсичного контента актуальна с самого основания сети Интернет. И, конечно, уже существует большое количество решений, помогающих в этом. Давайте рассмотрим наиболее распространенные из них.
 
\subsection{Perspective API}
  
Один из самых известных проектов на эту тему - разработка 2017 года, принадлежащая компании \textbf{Google} под названием \textbf{Perspective API}.

Авторы ставили перед собой задачу создать инструмент для модераторов, позволяющий обрабатывать комментарии на крупных платформах, таких как \textbf{Reddit}, \textbf{9gag}, и т.п. Среди партнеров также значатся крупные новостные агенства \textbf{The New York Times}, \textbf{The Guardian}, \textbf{The Economist}, и т.п.

К сожалению, мне не удалось найти подробное описание алгоритма, который используют разработчики этого сервиса. Вся общедоступная информация о технической части состоит только в том, какие корпусы текстов использовались для обучения модели.

\subsection{Scale Text Classification}

Сервис от компании \textbf{Scale} не направлен конкретно на работу с токсичным контентом. Однако, он считается одним из самых удачных в плане классификации на тематические классы. Таким образом, используя его, можно захватить все запрещенные к публикации группы контента.

Как и в предыдущем случае, код и алгоритмические наработки являются собственностью компании и закрыты для широкого пользования. Поэтому мы не будем долго останавливаться на нем.

\subsection{Соревнования на Kaggle}

Контест, организованный компанией \textbf{Quora} далеко не первый, связанный с классификацией токсичного контента. Как пример можно привести соревнование 2018 года от компании \textbf{Jigsaw}. Очень часто участники высоких мест выкладывают разборы своих решений, в которых рассказывают основное устройство модели и проблемы, с которыми они столкнулись в процессе исследования. Для нашей задачи полезно будет ознакомиться и разобраться в том, какие методики использовали участники, добившиеся наибольших результатов.

Я проанализировал несколько таких разборов, и пришел к выводу о том, что главный прирост в качестве дает использование словарей векторизации совместно с рекуррентными сетями. Таким образом мы можем заключить, что стоит подумать в направлении использования рекуррентных сетей применительно к текстам вопросов.

В целом, это довольно очевидный вывод, и хорошо, что разбор аналогичных задач подтвердил его.



\pagebreak





